{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.pipeline import Pipeline\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.base import BaseEstimator\n",
    "import torchmetrics.functional as metrics\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from classes.Patient import  Patient\n",
    "from classes.Dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'..\\data\\Healthy_15M_4753',# 135\n",
      "'..\\data\\Healthy_15M_4754',# 133\n",
      "'..\\data\\Healthy_4M_4240',# 95\n",
      "'..\\data\\Healthy_4M_4242',# 106\n",
      "'..\\data\\Healthy_4M_4243',# 146\n",
      "'..\\data\\Healthy_8M_4207',# 69\n",
      "'..\\data\\Healthy_8M_4208',# 122\n",
      "'..\\data\\Healthy_8M_4209',# 88\n",
      "'..\\data\\Sick_15M_4270',# 155\n",
      "'..\\data\\Sick_15M_4272',# 118\n",
      "'..\\data\\Sick_15M_4771',# 63\n",
      "'..\\data\\Sick_4M_4255',# 124\n",
      "'..\\data\\Sick_4M_4256',# 168\n",
      "'..\\data\\Sick_8M_3230',# 170\n",
      "'..\\data\\Sick_8M_3231',# 56\n",
      "'..\\data\\sigma\\Healthy_15M_4756',# 62\n",
      "'..\\data\\sigma\\Healthy_4M_4240',# 107\n",
      "'..\\data\\sigma\\Healthy_4M_4241',# 37\n",
      "'..\\data\\sigma\\Healthy_4M_42411',# 120\n",
      "'..\\data\\sigma\\Healthy_8M_4207',# 90\n",
      "'..\\data\\sigma\\Healthy_8M_4208',# 85\n",
      "'..\\data\\sigma\\Healthy_8M_4209',# 134\n",
      "'..\\data\\sigma\\Sick_15M_4272',# 89\n",
      "'..\\data\\sigma\\Sick_4M_4253',# 146\n",
      "'..\\data\\sigma\\Sick_4M_4254',# 153\n",
      "'..\\data\\sigma\\Sick_4M_4255',# 110\n",
      "'..\\data\\sigma\\Sick_4M_4256',# 82\n",
      "'..\\data\\sigma\\Sick_8M_3230',# 167\n",
      "'..\\data\\sigma\\Sick_8M_3231',# 113\n",
      "3243 29\n"
     ]
    }
   ],
   "source": [
    "x=0\n",
    "m=0\n",
    "path = Path('../data')\n",
    "for patient_dir in os.listdir(path):\n",
    "    if patient_dir == \"sigma\":\n",
    "        continue\n",
    "    m+=1\n",
    "    x += len(os.listdir(Path.joinpath(path, patient_dir)))\n",
    "    print(f\"\\'{Path.joinpath(path, patient_dir)}\\',#\", len(os.listdir(Path.joinpath(path, patient_dir))))\n",
    "    \n",
    "                             \n",
    "path = Path('../data/sigma')\n",
    "for patient_dir in os.listdir(path):\n",
    "    x += len(os.listdir(Path.joinpath(path, patient_dir)))\n",
    "    m+=1\n",
    "    print(f\"\\'{Path.joinpath(path, patient_dir)}\\',#\", len(os.listdir(Path.joinpath(path, patient_dir))))\n",
    "    \n",
    "\n",
    "print(x,m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "patient_paths = [\n",
    "'..\\data\\sigma\\Healthy_15M_4756',# 62\n",
    "'..\\data\\sigma\\Healthy_4M_4240',# 107\n",
    "'..\\data\\sigma\\Healthy_4M_4241',# 37\n",
    "'..\\data\\sigma\\Healthy_4M_42411',# 120\n",
    "'..\\data\\sigma\\Healthy_8M_4207',# 90\n",
    "'..\\data\\sigma\\Healthy_8M_4208',# 85\n",
    "'..\\data\\sigma\\Healthy_8M_4209',# 134\n",
    "'..\\data\\sigma\\Sick_15M_4272',# 89\n",
    "'..\\data\\sigma\\Sick_4M_4253',# 146\n",
    "'..\\data\\sigma\\Sick_4M_4254',# 153\n",
    "'..\\data\\sigma\\Sick_4M_4255',# 110\n",
    "'..\\data\\sigma\\Sick_4M_4256',# 82\n",
    "'..\\data\\sigma\\Sick_8M_3230',# 167\n",
    "'..\\data\\sigma\\Sick_8M_3231',# 113\n",
    "]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 62/62 [00:07<00:00,  7.89it/s]\n",
      "100%|██████████| 107/107 [00:13<00:00,  7.88it/s]\n",
      "100%|██████████| 37/37 [00:05<00:00,  6.56it/s]\n",
      "100%|██████████| 120/120 [00:19<00:00,  6.17it/s]\n",
      "100%|██████████| 90/90 [00:13<00:00,  6.80it/s]\n",
      "100%|██████████| 85/85 [00:10<00:00,  8.00it/s]\n",
      "100%|██████████| 134/134 [00:21<00:00,  6.31it/s]\n",
      "100%|██████████| 89/89 [00:10<00:00,  8.34it/s]\n",
      "100%|██████████| 146/146 [00:16<00:00,  9.02it/s]\n",
      "100%|██████████| 153/153 [00:16<00:00,  9.09it/s]\n",
      "100%|██████████| 110/110 [00:15<00:00,  6.91it/s]\n",
      "100%|██████████| 82/82 [00:12<00:00,  6.68it/s]\n",
      "100%|██████████| 167/167 [00:19<00:00,  8.36it/s]\n",
      "100%|██████████| 113/113 [00:10<00:00, 10.40it/s]\n"
     ]
    }
   ],
   "source": [
    "patients = [Patient(path_main_dir) for path_main_dir in patient_paths]\n",
    "for patient in patients:\n",
    "    patient.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(patients)\n",
    "y = np.array([x._class for x in patients])\n",
    "sss = ShuffleSplit(n_splits=20, test_size=0.33, random_state=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 93) (748,) (456, 93) (456,)\n",
      "(606, 93) (606,) (164, 93) (164,)\n",
      "(982, 93) (982,) (288, 93) (288,)\n",
      "(578, 93) (578,) (164, 93) (164,)\n",
      "(548, 93) (548,) (456, 93) (456,)\n",
      "(982, 93) (982,) (288, 93) (288,)\n",
      "(916, 93) (916,) (342, 93) (342,)\n",
      "(780, 93) (780,) (268, 93) (268,)\n",
      "(822, 93) (822,) (448, 93) (448,)\n",
      "(976, 93) (976,) (294, 93) (294,)\n",
      "(706, 93) (706,) (526, 93) (526,)\n",
      "(582, 93) (582,) (178, 93) (178,)\n",
      "(788, 93) (788,) (482, 93) (482,)\n",
      "(776, 93) (776,) (390, 93) (390,)\n",
      "(928, 93) (928,) (342, 93) (342,)\n",
      "(662, 93) (662,) (226, 93) (226,)\n",
      "(752, 93) (752,) (518, 93) (518,)\n",
      "(854, 93) (854,) (304, 93) (304,)\n",
      "(582, 93) (582,) (292, 93) (292,)\n"
     ]
    }
   ],
   "source": [
    "def preprocess(patients: np.ndarray, y: np.ndarray):\n",
    "    patients_dataset = Dataset(patients)\n",
    "    patients_dataset.concat_data()\n",
    "    \n",
    "    _df = pd.concat([x for x in patients_dataset.summary_image.summary_image_data])\n",
    "    y = _df['class']\n",
    "\n",
    "    _df['avg_dendrite_area'] = _df.apply(lambda x: x['AreaOccupied_AreaOccupied_secondary_dendrities'] / x['Count_secondary_dendrities'] if x['Count_secondary_dendrities'] != 0 else 0, axis=1)\n",
    "    _df = _df.drop(columns=['patient_path','Metadata_FileLocation','Frame_input_illum','FileName_input_illum','PathName_input_illum','URL_input_illum','class','ImageSet_ImageSet','MD5Digest_input_illum']).fillna(0)\n",
    "\n",
    "    undersample = RandomUnderSampler(sampling_strategy='majority')\n",
    "    X_df, y = undersample.fit_resample(_df, y)\n",
    "    return X_df, y\n",
    "\n",
    "def get_cross_valid_scaled_datasets(sss: ShuffleSplit, X, y):\n",
    "    for i, (train_index, test_index) in enumerate(sss.split(X, y)):\n",
    "        train_patients, y_train = X[train_index], y[train_index] \n",
    "        test_patients, y_test = X[test_index], y[test_index] \n",
    "       \n",
    "        if all(x == 0 for x in y_train) \\\n",
    "            or all(x == 1 for x in y_train) \\\n",
    "            or all(x == 0 for x in y_test) \\\n",
    "            or all(x == 1 for x in y_test):\n",
    "            continue\n",
    "        \n",
    "        X_train, y_train = preprocess(train_patients, y_train)\n",
    "        X_test, y_test = preprocess(test_patients, y_test)\n",
    "        \n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "        \n",
    "        yield X_train_scaled, y_train, X_test_scaled, y_test \n",
    "        \n",
    "for X_train, y_train, X_test, y_test in get_cross_valid_scaled_datasets(sss, X, y):\n",
    "    print(np.shape(X_train), np.shape(y_train), np.shape(X_test), np.shape(y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 48)\n",
    "        self.fc2 = nn.Linear(48, 24)\n",
    "        self.fc3 = nn.Linear(24, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "class MyDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset):\n",
    "        self.x, self.y = dataset        \n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        X = self.x[idx]\n",
    "        y = self.y[idx]\n",
    "        return X, y\n",
    "\n",
    "def train_eval_torch_model(model, X_train, y_train, X_test, y_test):\n",
    "    train_dataset = MyDataset((X_train, y_train.to_numpy()))\n",
    "    test_dataset = MyDataset((X_test, y_test.to_numpy()))\n",
    "    trainloader = torch.utils.data.DataLoader(train_dataset, batch_size=16)\n",
    "    testloader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "    #TRAIN\n",
    "    for epoch in range(20):  # loop over the dataset multiple times\n",
    "        for i, data in enumerate(trainloader, 0):\n",
    "            # get the inputs; data is a list of [inputs, labels]\n",
    "            inputs, labels = data\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            inputs = inputs.to(torch.float32)\n",
    "            # forward + backward + optimize\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # print statistics\n",
    "            # print(f'[{epoch + 1}, {i + 1:5d}] loss: {loss.item() :.3f}')\n",
    "\n",
    "    #EVAL\n",
    "    predictions = []\n",
    "    # since we're not training, we don't need to calculate the gradients for our outputs\n",
    "    with torch.no_grad():\n",
    "        for data in testloader:\n",
    "            images, labels = data\n",
    "            images = images.to(torch.float32)\n",
    "\n",
    "            # calculate outputs by running images through the network\n",
    "            outputs = model(images)\n",
    "            # print(outputs.data)\n",
    "            # the class with the highest energy is what we choose as prediction\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            # print(predicted)\n",
    "            predictions.append(predicted.cpu().detach().numpy().tolist())\n",
    "\n",
    "    pred_flatten = [item for row in predictions for item in row]\n",
    "    return torch.Tensor(pred_flatten)\n",
    "\n",
    "\n",
    "def train_eval_sklearn_model(model, X_train, y_train, X_test, y_test):\n",
    "    model.fit(X_train, y_train)\n",
    "    return torch.Tensor(model.predict(X_test))\n",
    "\n",
    "class MyModel:\n",
    "    def __init__(self, model) -> None:\n",
    "        self.model = model\n",
    "    \n",
    "    def train_eval_model(self):\n",
    "        if isinstance(self.model, Net):\n",
    "            return train_eval_torch_model\n",
    "        elif isinstance(self.model, BaseEstimator):\n",
    "            return train_eval_sklearn_model\n",
    "        else:\n",
    "            raise ValueError(\"Invalid model\")\n",
    "        \n",
    "\n",
    "    \n",
    "class Evaluator:\n",
    "    def __init__(self, X:np.ndarray(shape=len(X),dtype=Patient), y: np.ndarray(shape=len(y),dtype=str), n_splits: int):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.ss = ShuffleSplit(n_splits=n_splits, test_size=0.33, random_state=0)\n",
    "        \n",
    "\n",
    "    def prepare_models(self, num_features:int):\n",
    "        models = [\n",
    "            LogisticRegression(penalty='l1',solver='saga', C=0.7, max_iter=1000),\n",
    "            MLPClassifier(solver='sgd', alpha=1e1, hidden_layer_sizes=(24,12), random_state=1,verbose=False, learning_rate=\"adaptive\",learning_rate_init=0.001,early_stopping=True),\n",
    "            Net(num_features)\n",
    "        ]\n",
    "        self.models :List[MyModel] = [MyModel(model) for model in models]\n",
    "    \n",
    "    def prepare_metrics(self):\n",
    "        self.metrics_values = {}\n",
    "        self.metrics = [metrics.accuracy, metrics.recall, metrics.precision, metrics.f1_score] #, metrics.roc]\n",
    "        self.main_metric = metrics.accuracy.__name__\n",
    "        for model in self.models:\n",
    "            model_name = model.model.__name if hasattr(model, '__name__') else model.model.__class__.__name__\n",
    "\n",
    "            self.metrics_values[model_name] = {metric.__name__ if hasattr(metric, '__name__') else metric.__class__.__name__: [] for metric in self.metrics}\n",
    "\n",
    "\n",
    "    def calculate_metrics(self, model: str, y_true: torch.Tensor, y_pred: torch.Tensor) -> None:\n",
    "        for metric in self.metrics:\n",
    "            metric_name = metric.__name__ if hasattr(metric, '__name__') else metric.__class__.__name__\n",
    "            model_name = model.__name if hasattr(model, '__name__') else model.__class__.__name__\n",
    "\n",
    "            value = metric(torch.squeeze(y_pred), y_true.int(),\"binary\")\n",
    "            self.metrics_values[model_name][metric_name].append(value.item())\n",
    "\n",
    "\n",
    "    def run(self):\n",
    "        self.prepare_models(0)\n",
    "        self.prepare_metrics()\n",
    "\n",
    "        for X_train, y_train, X_test, y_test in get_cross_valid_scaled_datasets(self.ss, self.X, self.y):\n",
    "            print(np.shape(X_train), np.shape(y_train), np.shape(X_test), np.shape(y_test))\n",
    "            num_features = np.shape(X_train)[1]\n",
    "            self.prepare_models(num_features)\n",
    "            \n",
    "            for model in self.models:\n",
    "                train_fn = model.train_eval_model()\n",
    "                y_pred = train_fn(model.model, X_train, y_train, X_test, y_test)\n",
    "                self.calculate_metrics(model.model, torch.Tensor(y_test.values), y_pred)\n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\torch\\nn\\init.py:412: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(748, 93) (748,) (456, 93) (456,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(606, 93) (606,) (164, 93) (164,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(982, 93) (982,) (288, 93) (288,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(578, 93) (578,) (164, 93) (164,)\n",
      "(548, 93) (548,) (456, 93) (456,)\n",
      "(982, 93) (982,) (288, 93) (288,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(916, 93) (916,) (342, 93) (342,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(780, 93) (780,) (268, 93) (268,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(822, 93) (822,) (448, 93) (448,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(976, 93) (976,) (294, 93) (294,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(706, 93) (706,) (526, 93) (526,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(582, 93) (582,) (178, 93) (178,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(788, 93) (788,) (482, 93) (482,)\n",
      "(776, 93) (776,) (390, 93) (390,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(928, 93) (928,) (342, 93) (342,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(662, 93) (662,) (226, 93) (226,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(752, 93) (752,) (518, 93) (518,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(854, 93) (854,) (304, 93) (304,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(582, 93) (582,) (292, 93) (292,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\mainD\\study\\toolsTmp\\myAnaconda\\Anaconda\\envs\\codee\\lib\\site-packages\\sklearn\\linear_model\\_sag.py:352: ConvergenceWarning: The max_iter was reached which means the coef_ did not converge\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "evaluator = Evaluator(X, y, 20)\n",
    "evaluator.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           LogisticRegression  MLPClassifier       Net\n",
      "accuracy             0.746802       0.703850  0.748494\n",
      "recall               0.782756       0.734956  0.766672\n",
      "precision            0.756627       0.700312  0.775184\n",
      "f1_score             0.748307       0.696657  0.742820\n"
     ]
    }
   ],
   "source": [
    "avg_metrics = {}\n",
    "for model_name, model_metrics in evaluator.metrics_values.items():\n",
    "    avg_metrics[model_name] = {}\n",
    "    for metric_name, metric_value in model_metrics.items():\n",
    "        avg_metrics[model_name][metric_name] = np.mean(metric_value)\n",
    "print(pd.DataFrame(data=avg_metrics))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codee",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
